[
  {
    "objectID": "week_5.html#summary",
    "href": "week_5.html#summary",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThis week we were introduced to Google Earth Engine (GEE) which is a tool used as a geospatial processing service. The programming language for GEE is typically JavaScript, but there a Python API is available too, both of which can be used to create queryable applications.\nImportant Terms\n\nImage = a raster that has bands\nFeature = a vectors which has attributes and geometry\nImageCollection = image stack\nFeatureCollection = feature stack (lots of polygons)\n\n\n5.1.1 User Interface & Backend\nGEE has a two sides to its operation. The backend (aka the “Server side”) and the frontend (aka “User Interface”). The backend of GEE is where complex computations are processed as the server carries out tasks through using the var and ee. functions, and data collected from the chosen satellites are stored. However it is important to note that the server won’t show anything on the user interface unless Map.addLayer is used to call the variable into a layer. The frontend is the visual display of what code is being run and added to the map layers. Here legends, graphs and other feature can be added to showcase different aspects of what is being analysed. Polygons can be drawn or selected through entering coordinates and zoom levels can automatically be applied.\nThe image below is a screenshot of how GEE looks like in editor mode, as it displays the backend at the top of the screen, alongside a console to its right that indicates what processes have run, and the user interface below to display the layers that have been selected and called.\n\n\n\n\n\nImage source: Screenshot of Google Earth Engine Editor from local drive (2024)\n\n\n5.1.2 Scale\nIt is important to mention that GEE uses a top-down approach when rendering scale (aka resolution) of satellite imagery. As indicated in the image below, it layers each pixel of the chosen location, which is then reduced by region, or by neighbourhood where a specific parameter of the window would need to be indicated (aka specifying a kernel).\n\n\n\n\n\nImage source: Google Earth Engine (2024)"
  },
  {
    "objectID": "week_5.html#applications",
    "href": "week_5.html#applications",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nThere are many ways in which Google Earth Engine has been applied. According to its catalog, it has multiple datasets such as landcover, climate & weather, terrain, night-time imagery and more, which are available to researchers. The image below is an example of the Global Forest Watch which used GEE to “detect changes in forest cover”. Their dashboard has been extended to offer insight on landcover, climate and fires globally.\n\n\n\n\n\nImage source: Global Forest Watch (2014)\nAdditionally Wang et al. (2020) have noticed that there is a difference in GEE use between environmental and social remote sensing. Remote sensing of society can be used to analyse areas to perceive its development or for particular policies that are yet to be implemented, although agriculture is relatively significant too, whereas environmental analysis is more concerned with climate and physical land use.\n\n\n\n\n\nImage source: Wang et al. (2020)\nHeydari et al. (2024) conducted a study where they used GEE for drought predictions in Iran where they used several indicies. They extracted NDVI, along with temperature index values and had to validate their results, as part of processing data for predictions. Spectral and temporal filters were extracted in addition to spatial ones, and they found that pre-processed data was accessible. However, they did acknowledge certain limitation of GEE regarding processing and modelling, which were not extensively outlined, but were nevertheless able to perform necessary steps of their research."
  },
  {
    "objectID": "week_5.html#reflection",
    "href": "week_5.html#reflection",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nI really enjoyed working with GEE. I have never encountered anything like it. I particularly like how both the user interface and backend is simultaneously available to see, which makes it easier to organise the code and understand the visual outputs. The use of Google Earth Engine has plumeted in recent years, and its interesting to see the extend to which it can be applied to for analysis. I am keen to continue using GEE as I am sure it will become relevant at some point in the future. Although it is easier to map out the physical features in GEE, and somewhat harder to gain visualisations of social features, it does not discourage me from using it, as data can be imported from census and can be combined to produce certain output in the user interface. It is very beneficial to both researchers and academics to have an abundance of data to chose from and analyse, which I find particularly exciting as there are many datasets that can be combined for several purposes, both when analysing physical and human features of the world."
  },
  {
    "objectID": "week_5.html#references",
    "href": "week_5.html#references",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.4 References",
    "text": "5.4 References\nGoogle Earth Engine (2024) https://developers.google.com/earth-engine/guides/scale\nGlobal Forest Watch (2014) https://maps.googleblog.com/2014/02/monitoring-worlds-forests-with-global.html\nHeydani, H. et al. (2024) Innovative data clustering method improved drought prediction in heterogeneous landscapes using GEE-derived remote sensing indicies. Remote Sensing Applications: Society and Envrionment. 33, pg 1-24. [Online] Available via: https://www.sciencedirect.com/science/article/pii/S2352938523001945?casa_token=c1ICkOzPCiEAAAAA:jSqy7gbls-x_-NSwASr2zRkMRd2B33OtibS4Ck80w3O3WxwdFvBezO5-oPNQ37aoYmCp-2-Z\nWang, L. et al. (2020) A summary of the special issue on remote sensing of land change science with Google Earth Engine. Remote Sensing of Environment. 248, pg 1-9. [Online] Available via: https://www.sciencedirect.com/science/article/pii/S0034425720303722?casa_token=ZGNvttTlCr8AAAAA:QsF6p-ZoKnl742ohzFi_uQARdKgfXimXAYHo3sxsyBH09m4BmJRL615dkjCwGhhb8k_M02SE"
  },
  {
    "objectID": "week_4.html#summary",
    "href": "week_4.html#summary",
    "title": "4  Week 4 - Policy",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nFor this week’s task, I chose to look into Sydney’s Green Roof & Wall policy that was first introduced in 2014. The City of Sydney has already implemented this policy a decade ago, but has undergone slow progress which recorded 98,000m2 of green roofs and walls installed in March 2014, but currently this only equates to less than 1% of available roof space across the city (see bottom left image below). The local council set out a criteria that demands a minimum 30% vegetation cover to be installed in order to be considered as a “green roof”. 18% of Sydney’s central business district has potential to be retrofitted to accommodate the green roof policy.\n\n\nImage source of top two and bottom left photo: City of Sydney (2024)\nImage source of bottom right graph: Williams et al. (2021)\nThe bottom right image is taken from Williams et al. (2021) study which shows what types of buildings have implemented green roofs in Melbourne. When you compare this to the types of buildings listed to have either implemented a green roof or a green wall by the City of Sydney (bottom right image), it can assumed that Sydney also prioritised commercial buildings (in this case hotels), residential apartments and buildings such as 1 Bligh Street which is located in the central business district.\nThere are several benefits that are generated through the implementation of the green roof policy:\n\nAir quality - an increase in vegetation helps remove any harmful pollutants that may circulate the air.\nBiodiversity - can attract and support habitats of local fauna and flora\nInsulation - the multi-layering of different components that compose a green roof help to “reduce the reliance on active heating and cooling, which helps reduce energy consumption”.\nNoise - similar to the previous point, the layering and insulation provide and additional benefit of sound insulation, which reduce potential noise pollution from the surrounding active urban environment\nSpace - “previously unused space can be turned into valuable space for recreation, gardening, growing food etc”.\nRoof life - according to the local council, upgrading a roof through this strategy can extend a roof’s lifespan by up to 40 years. This is due to the fact that the extensive layering of a roof (as seen in the right-hand image above) limits the exposure to the Sun and harsh climate conditions.\nUrban heat island effect - green roofs help to combat this as the vegetative layer is known to reduce surface air temperatures."
  },
  {
    "objectID": "week_4.html#applications",
    "href": "week_4.html#applications",
    "title": "4  Week 4 - Policy",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nThere are several ways in which remote sensing can be implemented into Sydney’s Green Roof Policy. Although this policy was already implemented in 2014, it can be argued that there is scope to extend the project through detecting how many buildings can upgrade to a green roof, along side accommodating for the existing 18% of potential buildings that were detected prior to this. Additionally, the effects of green roofs on temperature reduction can be further studied by researcher when a significant amount of green roofs have been implemented in accordance to their policy aims towards a greener and sustainable city.\n\nMeasuring roof space - this can be done through using SAR (synthetic aperture radar) data, which can detect the housing footprints, as it extracts polygon data to obtain the square footage of each building that has the potential to install a green roof. These measurements can be compared to construction data to ensure the measurements recorded align with the extracted data from the satellite. Additionally, Bartesaghi-Koc et al. (2020) state that “LiDAR can be used to detect building footprints”.\nDetecting vegetation - the NDVI (Normalised Difference Vegetation Index) data can be extracted which will be able to detect the vegetation from existing buildings with implemented green roofs. Given that a minimum of 30% vegetation cover is required to meet the criteria from the city council, this can be paired up with roof measurements to detect which buildings are compatible with these requirements.\nTime-scale comparison - according to NASA Landsat Science (2024), GISS analysts used Landsat data to study how green roofs impacted temperature change in Chicago. Similarly, Sydney can use this data to present and compare before and after installing green roofs, and how increasing the scale of green roofs over time influence (ideally to reduce) temperatures, thus combating the urban heat island effect.\nTemperature change - Li et al. (2022) argues that this can be detected through several satellites such as Terra, Aqua and many more. LST (land surface temperature) can be measured and compared to a before and after period to test the extent of influence green roof implementation has on reducing temperatures. Additionally, Bartesaghi-Koc et al. (2020) conducted a study where they assessed NDVI data from summer and winter recordings using “Visible (RED) and Near Infrared (NIR) reflectance bands” to gain insight on whether compare tree cover results in cooling the temperature. Perhaps the same methods can be applied to detect the increase of vegetation and its impact on cooling the air temperature in order to reduce the urban heat island effect in Sydney."
  },
  {
    "objectID": "week_4.html#reflections",
    "href": "week_4.html#reflections",
    "title": "4  Week 4 - Policy",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nIn researching Sydney’s policy it was surprising to see how only less than 1% (53 buildings) have implemented green roofs to abide with the city’s policy. Although the policy does not directly state particular requirements in terms of quantity or building types that must adopt the policy, rather it states the benefits of implementing such green roofs and walls, it is still relatively shocking to see how little progress has been made, given that it has been a decade since its original installation. Perhaps there are several limitations to installing green roofs that may not be explicitly stated such as the cost of installation, availability of roofs and existing quality of roofs that may determine whether or not the buildings’ roof can sustain such upgrades.\nGiven that remote sensing enables the ease of detecting physical features within the environment, it was interesting to see how satellite data can be used for urban purposes. The policy promotes a solution whose main focus is designed towards tackling the urban heat island effect, but also produces social benefits such as generating usable space, reducing noise, and creating aesthetically appealing buildings which creates attraction towards a place. Although remote sensing cannot measure these social additions, it was interesting to see how it can measure the physical features such as vegetation, building footprints and air temperature to assess to what extent does implementing such policies create a positive environmental impacts, that would bring Sydney closer to achieving their sustainability goals."
  },
  {
    "objectID": "week_4.html#references",
    "href": "week_4.html#references",
    "title": "4  Week 4 - Policy",
    "section": "4.4 References",
    "text": "4.4 References\nBartesaghi-Koc, C. et al. (2020) Quantifying the second cooling capacity of ‘green infrastructure types’ (GITs): An approach to assess and mitigate surface urban heat island in Sydney, Australia. Landscape and Urban Planning. 203, pg 1-21. [Online] Available via: https://www.sciencedirect.com/science/article/pii/S0169204620301407\nCity of Syndey (2024) https://www.cityofsydney.nsw.gov.au/environmental-support-funding/green-roofs-and-walls\n\nGreen roofs and wall policy https://www.cityofsydney.nsw.gov.au/-/media/corporate/files/2020-07-migrated/files_g/green-roofs-and-walls-policy.pdf?download=true\nGreen roofs and wall policy implementation plan https://www.cityofsydney.nsw.gov.au/-/media/corporate/files/2020-07-migrated/files_g/green-roofs-and-walls-policy-implementation-plan-adopted.pdf?download=true\nGreen roofs and walls in the local area https://www.cityofsydney.nsw.gov.au/-/media/corporate/files/2020-07-migrated/files_m/map-green-roofs-walls-cos-lga2.pdf?download=true\n\nLi, Z. L. et al. (2022) Satellite Remote Sensing of Global Land Surface Temperature: Definition, Methods, Products and Applications. Reviews of Geophysics. 61(1), pg 1-77. [Online] Available via:https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2022RG000777\nNASA Landsat Science (2024) https://landsat.gsfc.nasa.gov/article/finding-ways-to-turn-down-the-heat-in-cities-with-satellites/\nWilliams, N.S.G et al. (2021) Ten years of greening a wide brown land: A synthesis of Australian green roof research and roadmap forward. Urban Forestry & Urban Greening. 62, pg 1-10. [Online] Available via: https://www.sciencedirect.com/science/article/pii/S1618866721002041"
  },
  {
    "objectID": "week_6.html#summary",
    "href": "week_6.html#summary",
    "title": "6  Week 6 - Classification 1",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nThis week we were introduced to a variety of concepts regarding classification. Many researchers have recognised that classified data can be used for identifying and measuring urban expansion, green spaces, air pollution, land surface tempertaure and much more. In this section I will focus on summarising CART and random forests, which are types of machine learning algorithms that can be applied to uncover patterns and solve regression and or classification errors.\n\n6.1.1 Classification and Regression Trees (CART)\nClassification trees allow you to classify data into two or more categories, whilst regression trees predict dependent variables that are continuous. The regression tree often has to subset data into smaller sections if it does not fit into a linear regression model. This is often quantified with the Gini impurity weighting using the following: 1-(probability of yes)^2-(probability of no)^2. Once this calculation is made, it it the lowest impurity that sits at the top of the decision making tree, (aka the root). The sum of the square residuals (SSR) is taken into account and checked against different thresholds. In order to prevent overfitting the model, a minimum no. of observations can be set.\n\n\n\n\n\nImage source: Medium. (2021)\n\n\n6.1.2 Overfitting\nThere are two ways in which overfitting can be prevented:\n\nlimit the no. of trees that are grown, this would require the minimum no. of pixels within a leaf to be set to 20\nWeakest leaf pruning - where a leaf is removed as the SSR increases\n\n\n\n\n\n\nImage source: Analyst Prep. (2021)\n\n\n6.1.3 Random Forests\nRandom forests are able to grow multiple classification trees and often generate decision trees from a random no. of variables. Bootstrapping (aka re-sampling by replacement) data is used to make a decision. In an out-of-bag sample (OOB), 70% of data is trained whilst 30% remains in bootstrap. The image below shows an example of how this process is carried out.\n\n\n\n\n\nImage source: Medium. (2020)\n\n\n6.1.4 Definitions\nBias - “the difference between predicted value and true value”\nVariance - “the variability of a model for a given point”"
  },
  {
    "objectID": "week_6.html#applications",
    "href": "week_6.html#applications",
    "title": "6  Week 6 - Classification 1",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nBelgiu & Dragut (2016) have sought to understand how random forests can be applied in remote sensing. They recognise it as a reliable method that can be applied as this classifier is able to successfully “select and rank those variables with the greatest ability to discriminate between the target classes” (pg 24), which enables to speed up an otherwise time-consuming process. They also acknowlegded that CART is also popular in remote sensing due to the lack of assumptions made around frequency distribution. Additionally Pal (2005) highlights several benefits of using random forest classification such as handling categorical data, even if values may be missing from the dataset, alongside being able to “detect outliers by using proximity analysis” (pg 221).\nFeng et al. (2015) conducted a study to map out vegetation in urban areas in China. They used random forest for image classification in the aims of achieving high accuracy levels. They set random predictor variables which where divided into training samples, and 1/3 of the the samples were OOB. This was done to “cross-validate and evaluate classification accuracy” (pg 1082). Based on the use of random forest, they were later able to perform texture analysis based on the classified results of land cover types, which can be seen in the image below.\n\nImage source: Feng et al. (2015) pg 1084"
  },
  {
    "objectID": "week_6.html#reflection",
    "href": "week_6.html#reflection",
    "title": "6  Week 6 - Classification 1",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nI found this weeks lecture quite overwhelming as there where several components covered. Despite this, it is still interesting to learn how machine learning algorithms can be applied in remote sensing. I was not aware of these processes prior to this lecture and was surprised by the extent of linear regression that is involved. It is clear that these machine learning techniques are beneficial to remote sensing processing as it is able to generate and classify scenarios which helps researchers identify which classes are best based on the ranking output that the random forest classifcation produces. It was interesting to see in the example above how implementing such techniques prove to be relevant in analysing urban vegetation, as this links back to the policy I investigated in week 4, and this could potentially be applied in the detection of green roofs, as it would look into similar features that Feng et al. (2015) had studied. Another thing that I found intriguing was to see how linear regression comes into play when measuring a model for overfitting. Gaining insight on how the data fits seems to provide an easy way to interpret the suitability of the classification that was previously conducted, which helps to direct the researchers to different techniques which can ensure its accuracy."
  },
  {
    "objectID": "week_6.html#references",
    "href": "week_6.html#references",
    "title": "6  Week 6 - Classification 1",
    "section": "6.4 References",
    "text": "6.4 References\nAnalyst Prep (2021) https://analystprep.com/study-notes/cfa-level-2/quantitative-method/overfitting-methods-addressing/\nBelgiu, M. & Dragut, L. (2016) Random forest in remote sensing, a review of applications and future direction. ISPRS Journal of Photogrammetry and Remote Sensing. 114, pg 24-31. [Online] Available via: https://www.sciencedirect.com/science/article/abs/pii/S0924271616000265\nFeng, Q. et al. (2015) UAV Remote Sensing for Urban Vegetation Mapping using Random Forest and Texture Analysis. Remote Sensing. 7(1), pg. 1074-1094. [Online] Available via: https://www.mdpi.com/2072-4292/7/1/1074\nMedium (2020) https://pub.towardsai.net/use-of-decision-trees-and-random-forest-in-machine-learning-1e35e737b638\nMedium (2021) https://medium.com/geekculture/decision-trees-with-cart-algorithm-7e179acee8ff\nPal, M. (2005) Random forest classifier for remote sensing classification. International Journal of Remote Sensing. 26(1), pg 217-222. [Online] Available via: https://www.tandfonline.com/doi/full/10.1080/01431160412331269698?casa_token=HgG9sL41P4QAAAAA%3A_NlexnqhGl6ZHvYeMiOdrM9ZLlJG9HdaGCuKUfo6fGlGZou4ZrAJzhEQtsS5uEsRJ5p5mMhm3y4"
  },
  {
    "objectID": "week_7.html#summary",
    "href": "week_7.html#summary",
    "title": "7  Week 7 - Classification 2",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis weeks lecture continued to outline additional classification methods that can be applied in remote sensing. Given the extent of content that was covered, I’ve decided to focus this weeks learning diary on OBIA,sub-pixel analysis and accuracy assessment.\n\n7.1.1 Object based image analysis (OBIA)\nThis technique takes the shapes (cells) into account based on either the similarity (homogeneity) or difference (heterogeneity) of the cells pixels. Often simple linear iterative clustering (SLIC) is used to calculate this as it measures the distance of a given point to the centre of the pixel. The image below compares the raw image on the left-hand side to the OBIA on the right. Here different objects, and thus land types can be identified; the red indicates buildings and residences whilst the green highlight the types of vegetation found within an area, differentiating between grass and trees.\n\n\n\n\n\nImage source: Land Info. (2024)\n\n\n7.1.2 Sub-pixel analysis\nThis technique combines percentage coverage of different classes and calculates the proportion of a particular landcover in a given pixel. Here reflectance is often measured by the “linear sum of end members that are weighted by the associated endmember fraction” (Lecture 8, slide 15)\n\n\n\n\n\nImage source: Ai et al. (2014)\nIn order to calculate sub-pixel analysis, the following equation should be applied:\n\n\n\n\n\n\npλ = pixel reflectance\npiλ = reflectance of end member i\nfi = fractional cover of end member i\nn = no. of end members\neλ = model error\n\n\n\n7.1.3 Accuracy assessment\nOnce an output is produced from any classification technique, an accuracy value needs to be assigned. Often it will undergo an accuracy assessment which compares the generated values between two classes tested. The producer accuracy (PA) is vertical. when the classification results meet the expectations of the creator. It often recalls true positive rate or sensitivity. The Users oe consumer’s accuracy (UA) are horizontal pixels that are incorrectly classified. OA is the overall accuracy.\nThere are four possible outcomes that this model can generate and have been listed below:\n\nTrue positive (TP) = the model predicts positive class correctly\nTrue negative (TN) = the model predicts negative class correctly\nFalse positive (FP) = the model predicts positive, but is it actually a negative class\nFalse negative (FN) = the model predicts negative, but it is actually a positive class.\n\nCalculations of different accuracy types: (Lecture 8, slide 23)\n\nProducers accuracy = “the fraction of correctly classified pixels compared to the ground truth data” -&gt; TP/TP+TN\nUser’s accuracy = “the fraction of correctly classified pixels relative to all other classifications” -&gt; (TP+FP) -&gt; TP/TP+FP\nOverall accuracy = “the combination of correctly classified pixels” -&gt; (TP+FP+FN+TN) -&gt; TP+TN/TP+FP+FN+TN\n\n\n\n\n\n\nImage source: Barsi et al. (2018)\n\n\n\n\n\nImage source: Medium. (2020)"
  },
  {
    "objectID": "week_7.html#applications",
    "href": "week_7.html#applications",
    "title": "7  Week 7 - Classification 2",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nAi et al. (2014) conducted a study where they performed sub-pixel analysis in addition to other methods, in order to map out different land cover types. The image below illustrates how land cover types are distributed in an urban area, which shows the concentration of urban infrastructure in relation to greenery and water access. In categorising different components for an urban system, it helps to distinguish the locations of certain land cover types, which can be useful in both urban planning and policy making.\n\nImage source: Ai et al. (2014)\nAdditionally Powell et al. (2007) conducted a study in Brazil using sub-pixel analysis from Landsat imagery, where they calculated average values for V-I-S (Vegetation-Impervious-Surface Soil) between 1996 to 1999. Where the impervious fraction is dense, the vegetation fraction almost visually displays the inverse in the images below. In addition to this, they conducted an accuracy assessment to validate the classifications previously made, that corresponded to each tested component. Despite the difference in visual outputs between the two studies, implementing the sub-pixel analysis ensures that categories can be corrcetly classified and tested, which allows each pixel to be assigned to different value depending on the type of land cover that is being assessed. Although. Powell et al. study goes into deeper analysis as they assess the V-I-S, Ai et al. still provide suitable outputs to portray the proportions of pixels with different types of categories. Both applications of this technique provide high-reoslution insight which can aid in differentiating spatial objects and geographical features from one another.\n\nImage source: Powell et al. (2007)"
  },
  {
    "objectID": "week_7.html#reflection",
    "href": "week_7.html#reflection",
    "title": "7  Week 7 - Classification 2",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nAlthough there was a lot of content to cover this week, it was interesting to see how spatial objects can be visually identifies from one another. I particularly enjoyed learning about sub-pixel analysis and the accuracy assessment. Although this is a relatively technical component, it is not too daunting to experiment with, as the theory sometimes seems more complicated than the practical side of it. It does take some time to fully grasp how to apply each technique, but it won’t discourage me from trying. It is clear that in remote sensing there are several techniques that have to be considered and applied in order to produce accurate outcomes which can be visually interpreted. Additionally, this proves how practical both GEE and remote sensing is as a tool, as it is able to perform multiple calculations which help classify different land types, among many other features. I look forward to experiment with these techniques in the future, as it has shown me that remote sensing can be applied widely to both physical and human geography, both of which are relevant to urban spatial analysis."
  },
  {
    "objectID": "week_7.html#references",
    "href": "week_7.html#references",
    "title": "7  Week 7 - Classification 2",
    "section": "7.4 References",
    "text": "7.4 References\nAi, B. et al. (2014) Improved sub-pixel mapping method coupling spatial dependence with directivity and connectivity. IEEE Journal of selected topics in Applied Earth Observations and Remote Sensing. 7(12), pg 4887- 4896. [Online] Available via: https://www.researchgate.net/publication/272183788_Improved_Sub-Pixel_Mapping_Method_Coupling_Spatial_Dependence_With_Directivity_and_Connectivity/download?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ\nBarsi, A. et al. (2018) Accuracy dimensions in remote sensing. ISPRS. 42(3), pg 61-67. [Online] Available via: https://isprs-archives.copernicus.org/articles/XLII-3/61/2018/isprs-archives-XLII-3-61-2018.pdf\nLand Info (2024) https://landinfo.com/image-classification-object-based-image-analysis-obia/\nMedium (2020) https://medium.com/(wenzhao.li1989/accuracy-assessment-d164e492274b?)\nPowell, R. L. et al. (2007) Sub-pixel mapping of urban land cover using multiple endmember spectral mixture analysis: Manaus, Brazil. Remote Sensing of Environment. 106(2), pg 253-267. [Online] Available via: https://www.sciencedirect.com/science/article/pii/S0034425706003142"
  },
  {
    "objectID": "week_8.html#summary",
    "href": "week_8.html#summary",
    "title": "8  Week 8 - Synthetic Aperture Radar (SAR)",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nAlthough synthetic aperture radar (SAR) was initially introduced in week 1, this week we went into more detail on how SAR operates and how it can be applied in remote sensing. SAR is known to be an active sensor that can observe the Earth through cloudy conditions and can detect during the night, both of which provide textured data, that can offer perspectives that other passive sensors cannot obtain.\nSAR is able to emit electromagnetic signals and record how that signal bounces back into the atmosphere. The satellite contains a long RADAR antenna that moves along the azimuth and sweeps the footprint of the Earth’s surface. The longer the antenna, the better the resolution of the images. One of the benefits of SAR is that it is able to record and compile images multiple times (aka “sweeping and moving”).\n\n\n\n\n\nImage source: NASA Earthdata. (2024)\n\n8.1.1 Polarisation & Amplitude (backscattering)\nIn remote sensing, polarisation is the “direction of travel of electromagnetic waves that are transmitted”. There are two orientation types: vertical (up and down) and horizontal (left to right). As indicated in the image below, polarisation is not responded to in the same manner across different surface types as the scattering is often determined by the wavelength. When water bodies are present, it\n\nRough scattering is most sensitive to VV, as it depends on the topology or surface or the Earth.\nVolume scattering bounce across surfaces (such as leaves in trees) and can be received as both VH and HV .\nDouble bounce scattering is mostly prominent in buildings, but sometime in trees too, which makes these objects sensitive to HH.\n\n\n\n\n\n\nImage source: NASA Earthdata. (2024)\nSAR signals have both backscatter (amplitude) and phase data. The phase data refers to the “location of the wave in a cycle when it returns to the sensor”. Here VV refers to the surface roughness, while VH refers to the volume of the surface.\n\n\n8.1.2 InSAR\nInSAR or Interferometric Sythetic Aperture Radar is a technique that “combines two or more SAR images” within the same region that can indicate either “surface motion or surface topography”. Applying this sort of method can be very useful in detection of physical changes over time such as earthquake detection, and the interferogram would output the extent of movement that occurs between the two images in a given location.\n\n\n\n\n\nImage source: NASA NISAR. (2024)"
  },
  {
    "objectID": "week_8.html#applications",
    "href": "week_8.html#applications",
    "title": "8  Week 8 - Synthetic Aperture Radar (SAR)",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nSAR detection can be applied in both physical and urban detection within remote sensing. One of the examples from Zhang et al. (2021) where they used Sentinel-1 polarmetric SAR images to detect flooded urban areas across two locations. Additionally, they pre-processed this data to “extract the backscattering coefficients and interferometric coherences” (pg7). Additionally they trained samples for both flooded and unflooded areas and compared the two results.\n\n\n\n\n\nImage source: Zhang et al. (2021)\nGens (2013) used SAR to detect oceaongraphic changes along the sea floor and he observed rain cells over the ocean. This application is quite different from what we have encountered in the lecture, as it focuses on water body analysis as opposed to land cover types and surfaces. He found that “Apart from wind gust effects on the sea surface, the radar backscatter strongly depends on the frequency and polarization of the radar signal.” (pg 294). His study acknowledged an issue regarding the temporal coverage of SAR when used for oceanogrpahic applications, as the “the difference in wavelength has to be factored in when data from different sources need to be combined in a synergistic analysis” (pg 296). Across both studies, SAR was able to detect changes in areas over time and location, and is able to identify surface types based on the wavelengths. It is clear that SAR is better for urban and physical land changes as opposed to ocean-based detection as there are less issues that are encountered and imagery provides more accurate results."
  },
  {
    "objectID": "week_8.html#reflection",
    "href": "week_8.html#reflection",
    "title": "8  Week 8 - Synthetic Aperture Radar (SAR)",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nI really enjoyed this weeks lecture, as we were able to go into more depth than we were originally introduced in the first week. I found that SAR can be broadly applied to various urban and rural areas that can analyse multiple features based on the polarisation and backscattering data that is collected. I find the concept of InSAR particularly interesting and would like to explore that in more detail in my spare time, as it seems like it would serve as a good temporal comparison. I particularly enjoyed learning about the different backscattering that the SAR sensor is able to pick up on, which has proven to be a useful tool in academic research. I would like to use SAR in my future line of work alongside GEE, as I believe it will aid in conducting visual and spatial analysis that is on trend with ongoing research."
  },
  {
    "objectID": "week_8.html#references",
    "href": "week_8.html#references",
    "title": "8  Week 8 - Synthetic Aperture Radar (SAR)",
    "section": "8.4 References",
    "text": "8.4 References\nGens (2013) Oceanographic. applications of SAR remote sensing. GIScience & Remote Sensing. 45(3), pg 275-305. [Online] Available via: https://www.tandfonline.com/doi/abs/10.2747/1548-1603.45.3.275\nNASA Earthdata (2024) https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar\nNASA NISAR (2024) https://nisar.jpl.nasa.gov/mission/get-to-know-sar/interferometry/\nZhang, H. et al. (2021) An Urban Flooding Index for unsupervised inundated urban area detection using Sentinel-1 polarmetric SAR images."
  },
  {
    "objectID": "week_1.html#summary",
    "href": "week_1.html#summary",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nThe concept of remote sensing is as clear as the name suggests; it is “sensing” ie: observing certain features of the Earth from a distance, often conducted through the aid of satellites. By exact definition it is: “the process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation at a distance” (USGS 2024).\nSpatial data scientists, among many other analysts, are taught to use this data which enables them to conduct analysis and gain geographical insight of a particular study area. This can involve topology, weather conditions, density of buildings, how developed an area is, and many more.\n\n1.1.1 Passive vs. Active Satellites\n\nThere are two types of satellites that have been designed for Earth Observation known to Remote Sensing. The diagrams illustrate the differences between the functions of these satellite types. Passive satellite sensors are designed to reflect energy from the Sun, where its’ energy is distributed and sensed as electromagnetic waves.\nExamples of passive sensors include the following:\n\nLandsat 8\nSentiel-2\nAqua (MODIS)\nPlanetscope (Dove)\nWorldview-4\nPleiades\n\n\n\n\n\n\nActive satellite sensors are designed to emit electromagnetic waves which then waits to receive information. These particular sensors are able to “see through” clouds, volcanic ash and other atmospheric conditions, and are also able to collect data during the night.\nExamples of active sensors include the following:\n\nSAR (Synthetic Apeture Radar)\nLiDAR (Light Detection and Ranging)\nSentiel-1\nRADARSAT-2\n\n\n\n1.1.2 Electromagnetic Radiation (EMR) - Waves\nElectromagnetic waves are three-dimensional in terms of the vertical movement of the electric field (blue), the horizontal movement of the magnetic field (pink) across a particular direction (as shown in the diagram below). These wavelengths come in different sizes creating an electromagnetic spectrum which can indicate the type of radiation emitted towards the Earth. Passive satellite sensors tend to operate in the visible, infrared, thermal infrared and microwave sections of the electromagnetic spectrum (NASA 2024), whilst active sensors operate on microwave and radio wavelength sections (ESA 2024).\n\nImage Source: created through Canva\n\n\n1.1.3 Atmospheric Scattering\nThe energy emitted from the Sun is scattered into the atmosphere as particles. The smaller wavelengths scatter across the Earth’s atmosphere, whilst longer wavelengths are absorbed and reflected back.\n\n\n\n\n\n\n\n1.1.4 Resolutions\nThere are four types of resolutions that need to be considered in remote sensing. One of the more obvious ones relate to spatial resolution which determines the size of each pixel in an image. In this way, you are selecting the geographic scale of the satellite image where lower range of meters implies a high-resolution image, as it is recorded from the closest capturing point (zoom level) to Earth observation, whilst the higher kilometer range indicated a lower-resolution image.\n\n\n\n\n\nThe other three definitions have been provided by NASA Earthdata (2024)\nSpectral - “the ability of a sensor to discern finer wavelengths”. This implies the resolution should have more or narrower bands.\nTemporal - “the time it take for a satellite to complete an orbit and revisit the same observation data”.\nRadiometric - “the amount of information in each pixel…the no. of bits representing the energy recorded”."
  },
  {
    "objectID": "week_1.html#applications",
    "href": "week_1.html#applications",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\n\nImage source: Viasat (2024)\nThe image above illustrates how different types of devices and technologies can operate on different wavelength frequencies, and thus correspond to a certain Hertz range. Satellites are able to capture different wavelengths, and this is determined on which bands are used for analysis.\n\n\n\n\n\nImage source: Viasat (2024)\nAccording to Patino & Duque (2013), the applications of remote sensing have advanced to benefit urban planning, as it can be implemented in research that involves the following: population density estimation, house-value modelling, informal settlement detection, land surface temperature, vegetation cover, soil quality detection etc, all of which can cause additional social impacts due to its relation to the urban environment. Much of this data obtained can be integrated into both local and national policies, which contribute to the urban development of a given area.\nZhao et al. (2019) reviewed nighttime light observation, which is enabled through observing active satellite sensors. Their study looked at the DMSP-OLS satellite (Defence Meteorological Satellite Programme - Operational Linescan System) where the photomultiplier tube (PMT) is used for detecting cities, fires, fishing boats and moonlit clouds. According to the NCEI (National Centre for Environmental Information), this type of satellite uses “visible and infrared sensors that collect images globally across a 3000km swath, twice a day”. This data also provides both solar and lunar information, which enables nighttime observations to be recorded according to the time period of intended detection. Despite the obvious benefit of this satellite being able to account for nighttime records, one of the drawback on this data is the “percent frequency of cloud-free light detections with no brightness information”. Zhao et al. (2019) have argued that this factor complicates the research of human activities during nighttime detection."
  },
  {
    "objectID": "week_1.html#reflection",
    "href": "week_1.html#reflection",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThere were a lot of interesting concepts introduced this week. I particularly enjoyed learning about the different types of sensors that operate on satellites, and the difference in types of resolution available to view and how this can alter the perspective of the area observed. I wasn’t aware of the amount of components that are require for remote sensing, despite the assumption of the complexity of satellites. I was particularly interested in discovering the applications of active sensors, as they can detect during the night and through clouds, which opens up a lot of research opportunities. I was interested in learning the different types of resolutions that are taken into account when processing satellite images.\nBased on the concepts introduced above, this made me realise that remote sensing can be a useful tool in spatial analysis as there are many features that can be used and adjusted such as the resolution type and scale that alter the way in which certain areas and geographical characteristics can be analysed. I don’t know yet whether I will be implementing remote sensing in my work in the future, but if in any case I do, I’m sure it would add an insightful perspective to my analysis which is equally visually stimulating."
  },
  {
    "objectID": "week_1.html#references",
    "href": "week_1.html#references",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\nESA (2024) https://www.esa.int/Education/7._Active_sensors#:~:text=The%20most%20common%20active%20sensor,regions%20of%20the%20electromagnetic%20spectrum.\nNASA (2024) https://www.earthdata.nasa.gov/learn/backgrounders/passive-sensors#:~:text=Most%20passive%20systems%20used%20in,portions%20of%20the%20electromagnetic%20spectrum.\nNASA Earthdata (2024) https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing#:~:text=Resolution%20plays%20a%20role%20in,spatial%2C%20spectral%2C%20and%20temporal.\nPatino, J.E. & Duque, J. C. (2013) A review if regional science applications of satellite remote sensing in urban setting. Computers, Environment and Urban Systems. 37, pg 1-17. [Available Online] Accessed via: https://www.sciencedirect.com/science/article/pii/S0198971512000567\nTime and Date (2024) https://www.timeanddate.com/astronomy/red-sunset.html\nViasat (2024) https://news.viasat.com/blog/scn/radio-waves-and-how-satellites-use-them\nZhao, M. et al. (2019) Applications of Satellite Remote Sensing of Nighttime Light Observations: Advances, Challenges and Perspectives. Remote Sensing. 11(17), pg 1-35. [Online] Available via: https://www.mdpi.com/2072-4292/11/17/1971"
  },
  {
    "objectID": "week_3.html#summary",
    "href": "week_3.html#summary",
    "title": "3  Week 3 - Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week focuses on corrections in remote sensing, often referring to the process which is applied to the raw satellite image. There are three types of corrections that can be used in remote sensing, and each poses a different solution to the type of image distortion encountered.\n\n3.1.1 Geometric correction\nThere are different types of errors that can occur when satellite data is being collected. In the case of this error, image distortions are at the forefront. For this type of correction, there are four potential image distortions that can be encountered: view angle, topography, wind and rotation of the Earth.\nIn order for this process to work, ground control points (GCPs) need to be identified in order to match points to the satellite image. Here the coordinates are modelled to produce geometric transformation coefficients. This is then plotted and we set the value to 0.5 in order to minimise the root-mean-square-deviation (RMSE).\nThere is another solution that can be applied for geometric correction. This involves the following calculation: (observed - predicted ((the residual))^2. Here the results will be summed up and divided by the no. of data points, and additionally, you calculate the square root of that result.\nThe image below illustrates how this process works.\n\n\n\n\n\nImage source: AWF Wiki (2024)\nThis indicates that the image may be re-sampled as a result. The following list provides ways in which re-sampling can be applied:\n\nLinear\nCubic\nCubic Spline\nNearest Neighbour\n\n\n\n3.1.2 Atmospheric correction\nAccording to GIS Geography (2023), atmospheric correction is the process which “removes the scattering and absorption effects from the atmosphere”. There are three types of atmospheric correction that can be applied:\n\nRelative (to something) correction aims to normalise the intensities of bands within a single image and from multiple dates to one. This can be achieved through the following processes:\n\nDark Object Subtraction (DOS) that searches for the darkest value in each band and subtracts it from each pixel.\nPseudo-Invariant Features (PIFs) that “assumes brightness pixels” of a base image and adjusts it according to the regression result.\n\nAbsolute (definitive) correction aims to change the brightness values into “scaled surface reflectance”, which is later compared to the surface reflectance. However, some datasets may require image altitude and atmospheric visibility data in order for this correction to be properly applied.\nEmpirical Line Correction which uses the measurements that derived form the linear regression against the satellite data. This involves the following calculation: Reflectance (fieldspectrum) = gain * radiance(input data) + offset\n\nThe image below shows the difference between satellite images before (left image - which shows “top of atmosphere reflectance”) and after (right image - which shows “surface reflectance”) atmospheric correction is applied. Often, “absorption and scattering creates haze in an image, which reduces the contrast of the image” (Lecture 3, slide 27) and this can be seen across the two images below.\n\n\n\n\n\nImage source: Mapbox (2013)\n\n\n3.1.3 Topographic correction\nThis type of correction is also know as Orthorectification Correction which is a subset of georectification. It is the process where distortions are removed through setting the viewpoint to the nadir, and in addition to that it provides coordinates to the image. This error occurs as “The topographical variations in the surface of the earth and the tilt of the satellite or aerial sensor can affect the distance with which features on the satellite or aerial image are displayed.” (Satellite Imaging Corporation (2022). The image below shows the layers applied when performing this correcton, which inidicates the requirement of an elevation model.\n\n\n\n\n\nImage source: Intermap (2019)\nAdditionally this solution requires the cosine correction model:\n\n\n\n\n\n\nLH is the “corrected radiance observed from the horizontal surface” - from DN to TOA (digital number to top of atmosphere)\nLT is the “radiance before correction observed from the inclined face”\ni is the “solar incidence angle” - the cosine of the angle between the normal line of the slope and the solar zenith\nθo (theta) is the “solar zenith angle”\n\n\n\n3.1.4 Radiometric calibration\nThis is where “sensors capture image brightness as digital number” through recording the “intensity of the electromagnetic radiation” (Humboldt State University 2020) and can be measured in the following ways:\n\nin watts (power or light)\nper square meter (surface within FOV)\nper steradian (angle of view)\nper nanometer (wavelength)\n\nList taken from Lecture 3, slide 48\n\n\n3.1.5 Definitions\nAzimuth angle - “the compass direction from which the sunlight is coming” that informs you which direction to face and can vary from 0˚ to 360˚\nGeorectification - giving coordinates to an image\nNadir - view of directly looking down on an area\nOrthorectification - removing distortions by making the pixels viewpoint at the nadir\nScattering - can create the “adjacency effect”, radiance from pixels nearby mixed into pixel of interest\nSlope - attenuated atmospheric correction which involved the “dimming and blurring from scattering of light”. This is the electromagnetic wave absorption and scattering by the atmosphere.\nSolar azimuth - compass angle of the Sun (N=0˚) 90˚E at sunrise and 270˚W at sunset\nSolar zenith - angle of local zenith (above the point on ground) and Sun from vertical (90˚ elevation)\nSpectral radiance - “the amount of light within a band from a sensor in the field ov view (FOV)”\nView angle - this means that the image angle doesn’t align with the Nadir."
  },
  {
    "objectID": "week_3.html#applications",
    "href": "week_3.html#applications",
    "title": "3  Week 3 - Corrections",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nIlori et al. (2019) conducted a study where they set out to investigate atmospheric correction techniques particularly for coastal remote sensing cases. They aimed to extract the ocean colour which “provides information on in-water optical properties” (pg 1), however this is difficult to extract as only “~10% of the total signal reaches the TOA” (pg 2). The main method that was applied to this study was atmospheric correction, but the researchers recognised that “residual errors can introduce large uncertainties in Rrs estimates, resulting in erroneous retrieval of OC products such as apparent optical properties” (pg 2). They found that the application of atmospheric correction was unreliable for “443 and 482 nm channels” but where able to perform well at”only a few sites located in nearshore and inland waters” (pg 12).\nMa et al. (2020) conducted a study to analyse uncertainty for topographic correction. They applied the cosine correction and the digital elevation model (DEM) in their methods and found that the uncertainty of the solar incidence angle was related to the rugged terrain, and its radiance substantially increased after this correction was applied. Despite these methods being different from each other, it is clear that there are other errors and uncertainties that can appear even after corrections have been applied to the satellite images.\n\n\n\n\n\nImage source: Ma et al. (2020)"
  },
  {
    "objectID": "week_3.html#reflection",
    "href": "week_3.html#reflection",
    "title": "3  Week 3 - Corrections",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nThis weeks topic was relatively interesting for me to learn about. I was not aware that there would be such a variety of errors and correction types that can be applied to satellite imagery. Having insight into correction methods seems important as it ensures that errors are removed to an extent and the quality and accuracy of the image are at the forefront. In this week, joining datasets and enhancements were also covered, both of which are equally important to what was summarised above, but I decided to focus on the correction types as I felt that this is a necessary step that needs to take place prior to analysing imagery for whatever research purpose, otherwise if this isn’t done, it may result in issues regarding the accuracy of data analysed which may further hinder and results derived from the imagery. I have never encountered any of this before, so it was quite overwhelming to grasp at first, as there are several sub-components to each correction method. However, I still consider this significant “preventive” that can aid any researcher when engaging in remote sensing analysis."
  },
  {
    "objectID": "week_3.html#references",
    "href": "week_3.html#references",
    "title": "3  Week 3 - Corrections",
    "section": "3.4 References",
    "text": "3.4 References\nAWK Wiki (2024) http://wiki.awf.forst.uni-goettingen.de/wiki/index.php/Geometric_corrections\nCelestis (2024) https://www.celestis.com/resources/faq/what-are-the-azimuth-and-elevation-of-a-satellite/#:~:text=Azimuth%20and%20Elevation%20are%20measures,0°%20to%20360°.\nGIS Geography (2023) https://gisgeography.com/atmospheric-correction/\nHumboldt State University (2020) http://gsp.humboldt.edu/olm/Courses/GSP_216/online/lesson7/radiometric.html\nIlori, C. O. et al. (2019) Analysing Performances of Different Atmospheric Correction Techniques for Landsat 8: Application for Coastal Remote Sensing. Remote Sensing. 11(4), pg 1-20. [Online] Available via: https://www.mdpi.com/2072-4292/11/4/469\nIntermap (2019) https://www.intermap.com/blog/orthorectification-in-a-nutshell\nMa, Z. et al. (2020) Uncertainty Analysis for Topographic Correction for Hyperspectral Remote Sensing Images. Remote Sensing. 12(4), pg 1-24. [Online] Available via: https://www.mdpi.com/2072-4292/12/4/705\nMapbox (2013) https://blog.mapbox.com/before-and-after-atmospheric-correction-97f55cb2b5d1\nSatellite Imaging Corporation (2022) https://www.satimagingcorp.com/services/orthorectification/#:~:text=The%20image%20data%20must%20be,angles%2C%20positions%2C%20and%20areas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RS - Quarto",
    "section": "",
    "text": "Introduction\nHello, I’m Maria! I was born and raised in West London. I graduated in BA Geography with Spatial Data Science Pathway in July 2023 from King’s College London, and moved straight onto my masters course in Urban Spatial Science MSc at UCL (University College London) at the Center for Advance Spatial Analytics. My academic interests have always lied in human geography, especially analysing behaviour patterns, influences and place interactions. This is particularly reflected through my dissertation where I studied how the interaction and exposure from social media platforms influence the ways in which places are explored. During my undergraduate degree, I took three modules in spatial data science, where we focused on learning the Python programming language, and this later encouraged me to continue this pathway and expand my technical skills.\nAside from academic interests, I have always loved to travel and explore places ever since I can remember, which is partially what inspired me to take geography in the first place. Alongside that, my dance trips have enabled me to visit different countries and cultures, enjoy good food, music and warm weather.\n\n\n\n\n\nAlthough it is not my first time coding, Remote Sensing is a relatively new topic for me, as it is not usually taken by human geographers. I am keen to learn about how these skills are applied in spatial analysis, and hopefully I can implement what I have learnt in the future!"
  }
]