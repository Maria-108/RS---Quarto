[
  {
    "objectID": "week_1.html#summary",
    "href": "week_1.html#summary",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nThe concept of remote sensing is as clear as the name suggests; it is “sensing” ie: observing certain features of the Earth from a distance, often conducted through the aid of satellites. By exact definition it is: “the process of detecting and monitoring the physical characteristics of an area by measuring its reflected and emitted radiation at a distance” (USGS 2024).\nSpatial data scientists, among many other analysts, are taught to use this data which enables them to conduct analysis and gain geographical insight of a particular study area. This can involve topology, weather conditions, density of buildings, how developed an area is, and many more.\n\n1.1.1 Passive vs. Active Satellites\n\nThere are two types of satellites that have been designed for Earth Observation known to Remote Sensing. The diagrams illustrate the differences between the functions of these satellite types. Passive satellite sensors are designed to reflect energy from the Sun, where its’ energy is distributed and sensed as electromagnetic waves.\nExamples of passive sensors include the following:\n\nLandsat 8\nSentiel-2\nAqua (MODIS)\nPlanetscope (Dove)\nWorldview-4\nPleiades\n\n\n\n\n\n\nActive satellite sensors are designed to emit electromagnetic waves which then waits to receive information. These particular sensors are able to “see through” clouds, volcanic ash and other atmospheric conditions, and are also able to collect data during the night.\nExamples of active sensors include the following:\n\nSAR (Synthetic Apeture Radar)\nLiDAR (Light Detection and Ranging)\nSentiel-1\nRADARSAT-2\n\n\n\n1.1.2 Electromagnetic Radiation (EMR) - Waves\nElectromagnetic waves are three-dimensional in terms of the vertical movement of the electric field (blue), the horizontal movement of the magnetic field (pink) across a particular direction (as shown in the diagram below). These wavelengths come in different sizes creating an electromagnetic spectrum which can indicate the type of radiation emitted towards the Earth. Passive satellite sensors tend to operate in the visible, infrared, thermal infrared and microwave sections of the electromagnetic spectrum (NASA 2024), whilst active sensors operate on microwave and radio wavelength sections (ESA 2024).\n\n\n\n\n\nImage Source: created through Canva\n\n\n1.1.3 Atmospheric Scattering\nThe energy emitted from the Sun is scattered into the atmosphere as particles. The smaller wavelengths scatter across the Earth’s atmosphere, whilst longer wavelengths are absorbed and reflected back.\n\n\n\n\n\n\n\n1.1.4 Resolutions\nThere are four types of resolutions that need to be considered in remote sensing. One of the more obvious ones relate to spatial resolution which determines the size of each pixel in an image. In this way, you are selecting the geographic scale of the satellite image where lower range of meters implies a high-resolution image, as it is recorded from the closest capturing point (zoom level) to Earth observation, whilst the higher kilometer range indicated a lower-resolution image.\n\n\n\n\n\nThe other three definitions have been provided by NASA Earthdata (2024)\nSpectral - “the ability of a sensor to discern finer wavelengths”. This implies the resolution should have more or narrower bands.\nTemporal - “the time it take for a satellite to complete an orbit and revisit the same observation data”.\nRadiometric - “the amount of information in each pixel…the no. of bits representing the energy recorded”."
  },
  {
    "objectID": "week_1.html#applications",
    "href": "week_1.html#applications",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\n\nImage source: Viasat (2024)\nThe image above illustrates how different types of devices and technologies can operate on different wavelength frequencies, and thus correspond to a certain Hertz range. Satellites are able to capture different wavelengths, and this is determined on which bands are used for analysis.\n\n\n\n\n\nImage source: Viasat (2024)\nAccording to Patino & Duque (2013), the applications of remote sensing have advanced to benefit urban planning, as it can be implemented in research that involves the following: population density estimation, house-value modelling, informal settlement detection, land surface temperature, vegetation cover, soil quality detection etc, all of which can cause additional social impacts due to its relation to the urban environment. Much of this data obtained can be integrated into both local and national policies, which contribute to the urban development of a given area.\nZhao et al. (2019) reviewed nighttime light observation, which is enabled through observing active satellite sensors. Their study looked at the DMSP-OLS satellite (Defence Meteorological Satellite Programme - Operational Linescan System) where the photomultiplier tube (PMT) is used for detecting cities, fires, fishing boats and moonlit clouds. According to the NCEI (National Centre for Environmental Information), this type of satellite uses “visible and infrared sensors that collect images globally across a 3000km swath, twice a day”. This data also provides both solar and lunar information, which enables nighttime observations to be recorded according to the time period of intended detection. Despite the obvious benefit of this satellite being able to account for nighttime records, one of the drawback on this data is the “percent frequency of cloud-free light detections with no brightness information”. Zhao et al. (2019) have argued that this factor complicates the research of human activities during nighttime detection."
  },
  {
    "objectID": "week_1.html#reflection",
    "href": "week_1.html#reflection",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nThere were a lot of interesting concepts introduced this week. I particularly enjoyed learning about the different types of sensors that operate on satellites, and the difference in types of resolution available to view and how this can alter the perspective of the area observed. I wasn’t aware of the amount of components that are require for remote sensing, despite the assumption of the complexity of satellites. I was particularly interested in discovering the applications of active sensors, as they can detect during the night and through clouds, which opens up a lot of research opportunities. I was interested in learning the different types of resolutions that are taken into account when processing satellite images.\nBased on the concepts introduced above, this made me realise that remote sensing can be a useful tool in spatial analysis as there are many features that can be used and adjusted such as the resolution type and scale that alter the way in which certain areas and geographical characteristics can be analysed. I don’t know yet whether I will be implementing remote sensing in my work in the future, but if in any case I do, I’m sure it would add an insightful perspective to my analysis which is equally visually stimulating."
  },
  {
    "objectID": "week_1.html#references",
    "href": "week_1.html#references",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.4 References",
    "text": "1.4 References\nESA (2024) https://www.esa.int/Education/7._Active_sensors#:~:text=The%20most%20common%20active%20sensor,regions%20of%20the%20electromagnetic%20spectrum.\nNASA (2024) https://www.earthdata.nasa.gov/learn/backgrounders/passive-sensors#:~:text=Most%20passive%20systems%20used%20in,portions%20of%20the%20electromagnetic%20spectrum.\nNASA Earthdata (2024) https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing#:~:text=Resolution%20plays%20a%20role%20in,spatial%2C%20spectral%2C%20and%20temporal.\nPatino, J.E. & Duque, J. C. (2013) A review if regional science applications of satellite remote sensing in urban setting. Computers, Environment and Urban Systems. 37, pg 1-17. [Available Online] Accessed via: https://www.sciencedirect.com/science/article/pii/S0198971512000567\nTime and Date (2024) https://www.timeanddate.com/astronomy/red-sunset.html\nViasat (2024) https://news.viasat.com/blog/scn/radio-waves-and-how-satellites-use-them\nZhao, M. et al. (2019) Applications of Satellite Remote Sensing of Nighttime Light Observations: Advances, Challenges and Perspectives. 2 11(17), pg 1-35. [Online] Available via: https://www.mdpi.com/2072-4292/11/17/1971"
  },
  {
    "objectID": "week_3.html#summary",
    "href": "week_3.html#summary",
    "title": "3  Week 3 - Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week focuses on corrections in remote sensing, often referring to the process which is applied to the raw satellite image. There are three types of corrections that can be used in remote sensing, and each poses a different solution to the type of image distortion encountered.\n\n3.1.1 Geometric correction\nThere are different types of errors that can occur when satellite data is being collected. In the case of this error, image distortions are at the forefront. For this type of correction, there are four potential image distortions that can be encountered: view angle, topography, wind and rotation of the Earth.\nIn order for this process to work, ground control points (GCPs) need to be identified in order to match points to the satellite image. Here the coordinates are modelled to produce geometric transformation coefficients. This is then plotted and we set the value to 0.5 in order to minimise the root-mean-square-deviation (RMSE).\nThere is another solution that can be applied for geometric correction. This involves the following calculation: (observed - predicted ((the residual))^2. Here the results will be summed up and divided by the no. of data points, and additionally, you calculate the square root of that result.\nThe image below illustrates how this process works.\n\n\n\n\n\nImage source: AWF Wiki (2024)\nThis indicates that the image may be re-sampled as a result. The following list provides ways in which re-sampling can be applied:\n\nLinear\nCubic\nCubic Spline\nNearest Neighbour\n\n\n\n3.1.2 Atmospheric correction\nAccording to GIS Geography (2023), atmospheric correction is the process which “removes the scattering and absorption effects from the atmosphere”. There are three types of atmospheric correction that can be applied:\n\nRelative (to something) correction aims to normalise the intensities of bands within a single image and from multiple dates to one. This can be achieved through the following processes:\n\nDark Object Subtraction (DOS) that searches for the darkest value in each band and subtracts it from each pixel.\nPseudo-Invariant Features (PIFs) that “assumes brightness pixels” of a base image and adjusts it according to the regression result.\n\nAbsolute (definitive) correction aims to change the brightness values into “scaled surface reflectance”, which is later compared to the surface reflectance. However, some datasets may require image altitude and atmospheric visibility data in order for this correction to be properly applied.\nEmpirical Line Correction which uses the measurements that derived form the linear regression against the satellite data. This involves the following calculation: Reflectance (fieldspectrum) = gain * radiance(input data) + offset\n\nThe image below shows the difference between satellite images before (left image - which shows “top of atmosphere reflectance”) and after (right image - which shows “surface reflectance”) atmospheric correction is applied. Often, “absorption and scattering creates haze in an image, which reduces the contrast of the image” (Lecture 3, slide 27) and this can be seen across the two images below.\n\n\n\n\n\nImage source: Mapbox (2013)\n\n\n3.1.3 Topographic correction\nThis type of correction is also know as Orthorectification Correction which is a subset of georectification. It is the process where distortions are removed through setting the viewpoint to the nadir, and in addition to that it provides coordinates to the image. This error occurs as “The topographical variations in the surface of the earth and the tilt of the satellite or aerial sensor can affect the distance with which features on the satellite or aerial image are displayed.” (Satellite Imaging Corporation (2022). The image below shows the layers applied when performing this correcton, which inidicates the requirement of an elevation model.\n\n\n\n\n\nImage source: Intermap (2019)\nAdditionally this solution requires the cosine correction model:\n\n\n\n\n\n\nLH is the “corrected radiance observed from the horizontal surface” - from DN to TOA (digital number to top of atmosphere)\nLT is the “radiance before correction observed from the inclined face”\ni is the “solar incidence angle” - the cosine of the angle between the normal line of the slope and the solar zenith\nθo (theta) is the “solar zenith angle”\n\n\n\n3.1.4 Radiometric calibration\nThis is where “sensors capture image brightness as digital number” through recording the “intensity of the electromagnetic radiation” (Humboldt State University 2020) and can be measured in the following ways:\n\nin watts (power or light)\nper square meter (surface within FOV)\nper steradian (angle of view)\nper nanometer (wavelength)\n\nList taken from Lecture 3, slide 48\n\n\n3.1.5 Definitions\nAzimuth angle - “the compass direction from which the sunlight is coming” that informs you which direction to face and can vary from 0˚ to 360˚\nGeorectification - giving coordinates to an image\nNadir - view of directly looking down on an area\nOrthorectification - removing distortions by making the pixels viewpoint at the nadir\nScattering - can create the “adjacency effect”, radiance from pixels nearby mixed into pixel of interest\nSlope - attenuated atmospheric correction which involved the “dimming and blurring from scattering of light”. This is the electromagnetic wave absorption and scattering by the atmosphere.\nSolar azimuth - compass angle of the Sun (N=0˚) 90˚E at sunrise and 270˚W at sunset\nSolar zenith - angle of local zenith (above the point on ground) and Sun from vertical (90˚ elevation)\nSpectral radiance - “the amount of light within a band from a sensor in the field ov view (FOV)”\nView angle - this means that the image angle doesn’t align with the Nadir."
  },
  {
    "objectID": "week_3.html#applications",
    "href": "week_3.html#applications",
    "title": "3  Week 3 - Corrections",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nIlori et al. (2019) conducted a study where they set out to investigate atmospheric correction techniques particularly for coastal remote sensing cases. They aimed to extract the ocean colour which “provides information on in-water optical properties” (pg 1), however this is difficult to extract as only “~10% of the total signal reaches the TOA” (pg 2). The main method that was applied to this study was atmospheric correction, but the researchers recognised that “residual errors can introduce large uncertainties in Rrs estimates, resulting in erroneous retrieval of OC products such as apparent optical properties” (pg 2). They found that the application of atmospheric correction was unreliable for “443 and 482 nm channels” but where able to perform well at”only a few sites located in nearshore and inland waters” (pg 12).\nMa et al. (2020) conducted a study to analyse uncertainty for topographic correction. They applied the cosine correction and the digital elevation model (DEM) in their methods and found that the uncertainty of the solar incidence angle was related to the rugged terrain, and its radiance substantially increased after this correction was applied. Despite these methods being different from each other, it is clear that there are other errors and uncertainties that can appear even after corrections have been applied to the satellite images.\n\n\n\n\n\nImage source: Ma et al. (2020)"
  },
  {
    "objectID": "week_3.html#reflection",
    "href": "week_3.html#reflection",
    "title": "3  Week 3 - Corrections",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nThis weeks topic was relatively interesting for me to learn about. I was not aware that there would be such a variety of errors and correction types that can be applied to satellite imagery. Having insight into correction methods seems important as it ensures that errors are removed to an extent and the quality and accuracy of the image are at the forefront. In this week, joining datasets and enhancements were also covered, both of which are equally important to what was summarised above, but I decided to focus on the correction types as I felt that this is a necessary step that needs to take place prior to analysing imagery for whatever research purpose, otherwise if this isn’t done, it may result in issues regarding the accuracy of data analysed which may further hinder and results derived from the imagery. I have never encountered any of this before, so it was quite overwhelming to grasp at first, as there are several sub-components to each correction method. However, I still consider this significant “preventive” that can aid any researcher when engaging in remote sensing analysis."
  },
  {
    "objectID": "week_3.html#references",
    "href": "week_3.html#references",
    "title": "3  Week 3 - Corrections",
    "section": "3.4 References",
    "text": "3.4 References\nAWK Wiki (2024) http://wiki.awf.forst.uni-goettingen.de/wiki/index.php/Geometric_corrections\nCelestis (2024) https://www.celestis.com/resources/faq/what-are-the-azimuth-and-elevation-of-a-satellite/#:~:text=Azimuth%20and%20Elevation%20are%20measures,0°%20to%20360°.\nGIS Geography (2023) https://gisgeography.com/atmospheric-correction/\nHumboldt State University (2020) http://gsp.humboldt.edu/olm/Courses/GSP_216/online/lesson7/radiometric.html\nIlori, C. O. et al. (2019) Analysing Performances of Different Atmospheric Correction Techniques for Landsat 8: Application for Coastal Remote Sensing. Remote Sensing. 11(4), pg 1-20. [Online] Available via: https://www.mdpi.com/2072-4292/11/4/469\nIntermap (2019) https://www.intermap.com/blog/orthorectification-in-a-nutshell\nMa, Z. et al. (2020) Uncertainty Analysis for Topographic Correction for Hyperspectral Remote Sensing Images. Remote Sensing. 12(4), pg 1-24. [Online] Available via: https://www.mdpi.com/2072-4292/12/4/705\nMapbox (2013) https://blog.mapbox.com/before-and-after-atmospheric-correction-97f55cb2b5d1\nSatellite Imaging Corporation (2022) https://www.satimagingcorp.com/services/orthorectification/#:~:text=The%20image%20data%20must%20be,angles%2C%20positions%2C%20and%20areas."
  },
  {
    "objectID": "week_5.html#summary",
    "href": "week_5.html#summary",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThis week we were introduced to Google Earth Engine (GEE) which is a tool used as a geospatial processing service. The programming language for GEE is typically JavaScript, but there a Python API is available too, both of which can be used to create queryable applications.\nImportant Terms\n\nImage = a raster that has bands\nFeature = a vectors which has attributes and geometry\nImageCollection = image stack\nFeatureCollection = feature stack (lots of polygons)\n\n\n5.1.1 User Interface & Backend\nGEE has a two sides to its operation. The backend (aka the “Server side”) and the frontend (aka “User Interface”). The backend of GEE is where complex computations are processed as the server carries out tasks through using the var and ee. functions, and data collected from the chosen satellites are stored. However it is important to note that the server won’t show anything on the user interface unless Map.addLayer is used to call the variable into a layer. The frontend is the visual display of what code is being run and added to the map layers. Here legends, graphs and other feature can be added to showcase different aspects of what is being analysed. Polygons can be drawn or selected through entering coordinates and zoom levels can automatically be applied.\nThe image below is a screenshot of how GEE looks like in editor mode, as it displays the backend at the top of the screen, alongside a console to its right that indicates what processes have run, and the user interface below to display the layers that have been selected and called.\n\n\n\n\n\nImage source: Screenshot of Google Earth Engine Editor from local drive (2024)\n\n\n5.1.2 Scale\nIt is important to mention that GEE uses a top-down approach when rendering scale (aka resolution) of satellite imagery. As indicated in the image below, it layers each pixel of the chosen location, which is then reduced by region, or by neighbourhood where a specific parameter of the window would need to be indicated (aka specifying the kernel).\n\n\n\n\n\nImage source: Google Earth Engine (2024)"
  },
  {
    "objectID": "week_5.html#applications",
    "href": "week_5.html#applications",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nThere are many ways in which Google Earth Engine has been applied. According to its catalog, it has multiple datasets such as landcover, climate & weather, terrain, night-time imagery and more, which are available to researchers. The image below is an example of the Global Forest Watch which used GEE to “detect changes in forest cover”. Their dashboard has been extended to offer insight on landcover, climate and fires globally.\n\n\n\n\n\nImage source: Global Forest Watch (2014)\nAdditionally Wang et al. (2020) have noticed that there is a difference in GEE use between environmental and social remote sensing. Remote sensing of society can be used to analyse areas to perceive its development or for particular policies that are yet to be implemented, although agriculture is relatively significant too, whereas environmental analysis is more concerned with climate and physical land use.\n\n\n\n\n\nImage source: Wang et al. (2020)\nHeydari et al. (2024) conducted a study where they used GEE for drought predictions in Iran where they used several indicies. They extracted NDVI, along with temperature index values and had to validate their results, as part of processing data for predictions. Spectral and temporal filters were extracted in addition to spatial ones, and they found that pre-processed data was accessible. However, they did acknowledge certain limitation of GEE regarding processing and modelling, which were not extensively outlined, but were nevertheless able to perform necessary steps of their research."
  },
  {
    "objectID": "week_5.html#reflection",
    "href": "week_5.html#reflection",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nI really enjoyed working with GEE. I have never encountered anything like it. I particularly like how both the user interface and backend is simultaneously available to see, which makes it easier to organise the code and understand the visual outputs. The use of Google Earth Engine has plumeted in recent years, and its interesting to see the extend to which it can be applied to for analysis. I am keen to continue using GEE as I am sure it will become relevant at some point in the future. Although it is easier to map out the physical features in GEE, and somewhat harder to gain visualisations of social features, it does not discourage me from using it, as data can be imported from census and can be combined to produce certain output in the user interface. It is very beneficial to both researchers and academics to have an abundance of data to chose from and analyse, which I find particularly exciting as there are many datasets that can be combined for several purposes, both when analysing physical and human features of the world."
  },
  {
    "objectID": "week_5.html#references",
    "href": "week_5.html#references",
    "title": "5  Week 5 - Google Earth Engine (GEE)",
    "section": "5.4 References",
    "text": "5.4 References\nGoogle Earth Engine (2024) https://developers.google.com/earth-engine/guides/scale\nGlobal Forest Watch (2014) https://maps.googleblog.com/2014/02/monitoring-worlds-forests-with-global.html\nHeydani, H. et al. (2024) Innovative data clustering method improved drought prediction in heterogeneous landscapes using GEE-derived remote sensing indicies. Remote Sensing Applications: Society and Envrionment. 33, pg 1-24. [Online] Available via: https://www.sciencedirect.com/science/article/pii/S2352938523001945?casa_token=c1ICkOzPCiEAAAAA:jSqy7gbls-x_-NSwASr2zRkMRd2B33OtibS4Ck80w3O3WxwdFvBezO5-oPNQ37aoYmCp-2-Z\nWang, L. et al. (2020) A summary of the special issue on remote sensing of land change science with Google Earth Engine. Remote Sensing of Environment. 248, pg 1-9. [Online] Available via: https://www.sciencedirect.com/science/article/pii/S0034425720303722?casa_token=ZGNvttTlCr8AAAAA:QsF6p-ZoKnl742ohzFi_uQARdKgfXimXAYHo3sxsyBH09m4BmJRL615dkjCwGhhb8k_M02SE"
  }
]